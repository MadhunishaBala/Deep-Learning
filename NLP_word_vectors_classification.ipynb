{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification using Neural Networks\n",
    "\n",
    "The goal of this notebook is to learn to use Neural Networks for text classification.\n",
    "\n",
    "In this notebook, we will:\n",
    "- Train a shallow model with learning embeddings\n",
    "- Download pre-trained embeddings from Glove\n",
    "- Use these pre-trained embeddings\n",
    "\n",
    "However keep in mind:\n",
    "- Deep Learning can be better on text classification that simpler ML techniques, but only on very large datasets and well designed/tuned models.\n",
    "- We won't be using the most efficient (in terms of computing) techniques, as Keras is good for prototyping but rather inefficient for training small embedding models on text.\n",
    "- The following projects can replicate similar word embedding models much more efficiently: [word2vec](https://github.com/dav/word2vec) and [gensim's word2vec](https://radimrehurek.com/gensim/models/word2vec.html)   (self-supervised learning only), [fastText](https://github.com/facebookresearch/fastText) (both supervised and self-supervised learning), [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/wiki) (supervised learning).\n",
    "- Plain shallow sparse TF-IDF bigrams features without any embedding and Logistic Regression or Multinomial Naive Bayes is often competitive in small to medium datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The BBC topic classification dataset\n",
    "\n",
    "The BBC provides some benchmark topic classification datasets in English at: http://mlg.ucd.ie/datasets/bbc.html.\n",
    "\n",
    "The raw text (encoded with the latin-1 character encoding) of the news can be downloaded as a ZIP archive:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:05:52.186573Z",
     "start_time": "2024-10-24T16:05:52.175075Z"
    }
   },
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import zipfile\n",
    "try:\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "\n",
    "BBC_DATASET_URL = \"http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip\"\n",
    "zip_filename = BBC_DATASET_URL.rsplit('/', 1)[1]\n",
    "BBC_DATASET_FOLDER = 'bbc'\n",
    "if not op.exists(zip_filename):\n",
    "    print(\"Downloading %s to %s...\" % (BBC_DATASET_URL, zip_filename))\n",
    "    urlretrieve(BBC_DATASET_URL, zip_filename)\n",
    "\n",
    "if not op.exists(BBC_DATASET_FOLDER):\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as f:\n",
    "        print(\"Extracting contents of %s...\" % zip_filename)\n",
    "        f.extractall('.')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the five folders contains text files from one of the five topics:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:05:52.278667Z",
     "start_time": "2024-10-24T16:05:52.265988Z"
    }
   },
   "source": [
    "target_names = sorted(folder for folder in os.listdir(BBC_DATASET_FOLDER)\n",
    "                      if op.isdir(op.join(BBC_DATASET_FOLDER, folder)))\n",
    "target_names"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['business', 'entertainment', 'politics', 'sport', 'tech']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomly partition the text files in a training and test set while recording the target category of each file as an integer:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:05:54.380943Z",
     "start_time": "2024-10-24T16:05:52.338311Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = []\n",
    "filenames = []\n",
    "for target_id, target_name in enumerate(target_names):\n",
    "    class_path = op.join(BBC_DATASET_FOLDER, target_name)\n",
    "    for filename in sorted(os.listdir(class_path)):\n",
    "        filenames.append(op.join(class_path, filename))\n",
    "        target.append(target_id)\n",
    "\n",
    "target = np.asarray(target, dtype=np.int32)\n",
    "target_train, target_test, filenames_train, filenames_test = train_test_split(\n",
    "    target, filenames, test_size=200, random_state=0)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:05:54.462581Z",
     "start_time": "2024-10-24T16:05:54.452600Z"
    }
   },
   "source": [
    "len(target_train), len(filenames_train)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2025, 2025)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:05:54.582156Z",
     "start_time": "2024-10-24T16:05:54.571325Z"
    }
   },
   "source": [
    "len(target_test), len(filenames_test)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that text of some document have been loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:05:54.692364Z",
     "start_time": "2024-10-24T16:05:54.685739Z"
    }
   },
   "source": [
    "idx = 0\n",
    "\n",
    "with open(filenames_train[idx], 'rb') as f:\n",
    "    print(\"class:\", target_names[target_train[idx]])\n",
    "    print()\n",
    "    print(f.read().decode('latin-1')[:500] + '...')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: business\n",
      "\n",
      "Watchdog probes Vivendi bond sale\n",
      "\n",
      "French stock market regulator AMF has filed complaints against media giant Vivendi Universal, its boss and another top executive.\n",
      "\n",
      "It believes the prospectus for a bond issue was unclear and that executives may have had privileged information. AMF has begun proceedings against Vivendi, its chief executive Jean-Rene Fourtou and chief operating officer Jean-Bernard Levy. Vivendi advisor Deutsche Bank was also the subject of a complaint filing. Deutsche Bank, whic...\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:05:55.163342Z",
     "start_time": "2024-10-24T16:05:54.798800Z"
    }
   },
   "source": [
    "size_in_bytes = sum([len(open(fn, 'rb').read()) for fn in filenames_train])\n",
    "print(\"Training set size: %0.3f MB\" % (size_in_bytes / 1e6))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 4.582 MB\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is small so we can preload it all in memory once and for all to simplify the notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:05:55.496586Z",
     "start_time": "2024-10-24T16:05:55.182048Z"
    }
   },
   "source": [
    "texts_train = [open(fn, 'rb').read().decode('latin-1') for fn in filenames_train]\n",
    "texts_test = [open(fn, 'rb').read().decode('latin-1') for fn in filenames_test]"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first baseline model\n",
    "\n",
    "For simple topic classification problems, one should always try a simple method first. In this case a good baseline is extracting TF-IDF normalized bag of bi-grams features and then use a simple linear classifier such as logistic regression.\n",
    "\n",
    "It's a very efficient method and should give us a strong baseline to compare our deep learning method against."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:05:55.684488Z",
     "start_time": "2024-10-24T16:05:55.515935Z"
    }
   },
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "text_classifier = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3, max_df=0.8, ngram_range=(1, 2)),\n",
    "    LogisticRegression(),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:05:59.796136Z",
     "start_time": "2024-10-24T16:05:55.722664Z"
    }
   },
   "source": [
    "%time _ = text_classifier.fit(texts_train, target_train)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.19 s\n",
      "Wall time: 4.05 s\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:06:00.097924Z",
     "start_time": "2024-10-24T16:05:59.824765Z"
    }
   },
   "source": [
    "text_classifier.score(texts_test, target_test)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 classification errors on 200 test documents for a model fit in less than 10s. It's quite unlikely that we can significantly beat that baseline with a more complex deep learning based model. However let's try to reach a comparable level of accuracy with Embeddings-based models just for teaching purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text for the (supervised) CBOW model\n",
    "\n",
    "We will implement a simple classification model in Keras. Raw text requires (sometimes a lot of) preprocessing.\n",
    "\n",
    "The following cells uses Keras to preprocess text:\n",
    "- using a tokenizer. You may use different tokenizers (from scikit-learn, NLTK, custom Python function etc.). This converts the texts into sequences of indices representing the `20000` most frequent words\n",
    "- sequences have different lengths, so we pad them (add 0s at the end until the sequence is of length `1000`)\n",
    "- we convert the output classes as 1-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:06:14.112302Z",
     "start_time": "2024-10-24T16:06:00.186233Z"
    }
   },
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, char_level=False)\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "sequences = tokenizer.texts_to_sequences(texts_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Found 30995 unique tokens.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenized sequences are converted to list of token ids (with an integer code):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-10-24T16:06:14.129899Z",
     "start_time": "2024-10-24T16:06:14.123573Z"
    }
   },
   "source": [
    "sequences[0][:10]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1857, 9454, 5251, 1973, 452, 543, 577, 121, 3073, 9455]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer object stores a mapping (vocabulary) from word strings to token ids that can be inverted to reconstruct the original message (without formatting):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:06:14.228940Z",
     "start_time": "2024-10-24T16:06:14.176712Z"
    }
   },
   "source": [
    "type(tokenizer.word_index), len(tokenizer.word_index)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, 30995)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:06:14.306972Z",
     "start_time": "2024-10-24T16:06:14.291631Z"
    }
   },
   "source": [
    "index_to_word = dict((i, w) for w, i in tokenizer.word_index.items())"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:06:14.394070Z",
     "start_time": "2024-10-24T16:06:14.387583Z"
    }
   },
   "source": [
    "\" \".join([index_to_word[i] for i in sequences[0]])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"watchdog probes vivendi bond sale french stock market regulator amf has filed complaints against media giant vivendi universal its boss and another top executive it believes the prospectus for a bond issue was unclear and that executives may have had privileged information amf has begun proceedings against vivendi its chief executive jean rene fourtou and chief operating officer jean bernard levy vivendi advisor deutsche bank was also the subject of a complaint filing deutsche bank which was responsible for selling the convertible bonds to investors could face penalties if the complaint is upheld vivendi has said it believes there is no legal basis for the complaints the watchdog is said to believe the executive pair were party to privileged information surrounding the issue of the bonds both men bought some of the bonds the associated press news agency reported amf is investigating claims that the duo were aware of an interest in vivendi's us assets from investor marvin davis at the time of the bond sale vivendi however has said that the information was public knowledge as mr davis' offer for the us assets had already been rejected by vivendi's board amf is also looking into whether the executives knew that vivendi was considering exercising its right to buy british telecom's shares in cegetel vivendi has rejected the charge saying the decision to buy the cegetel shares was no more than a possibility of which the public was perfectly aware at the time of the bond issue back in december vivendi and its former chief executive jean marie messier were each fined 1m euros 1 3m â£690 000 by amf the fines came after a 15 month probe into allegations that the media giant misled investors after a costly acquisition programme went wrong\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at the tokenized sequences:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:06:14.481173Z",
     "start_time": "2024-10-24T16:06:14.473742Z"
    }
   },
   "source": [
    "seq_lens = [len(s) for s in sequences]\n",
    "print(\"average length: %0.1f\" % np.mean(seq_lens))\n",
    "print(\"max length: %d\" % max(seq_lens))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average length: 382.6\n",
      "max length: 4355\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:06:15.283699Z",
     "start_time": "2024-10-24T16:06:14.538325Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(seq_lens, bins=50);"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's zoom on the distribution of regular sized posts. The vast majority of the posts have less than 1000 symbols:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:06:15.630441Z",
     "start_time": "2024-10-24T16:06:15.331092Z"
    }
   },
   "source": [
    "plt.hist([l for l in seq_lens if l < 3000], bins=50);"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's truncate and pad all the sequences to 1000 symbols to build the training set:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T16:06:15.767650Z",
     "start_time": "2024-10-24T16:06:15.688801Z"
    }
   },
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "# pad sequences with 0s\n",
    "x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', x_train.shape)\n",
    "print('Shape of data test tensor:', x_test.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (2025, 1000)\n",
      "Shape of data test tensor: (200, 1000)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:12:22.627168Z",
     "start_time": "2024-10-25T18:12:22.592299Z"
    }
   },
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(target_train)\n",
    "y_test = to_categorical(target_test)\n",
    "\n",
    "print('Shape of label tensor:', y_train.shape)\n",
    "print('Shape of label tensor (test):', y_test.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (2025, 5)\n",
      "Shape of label tensor (test): (200, 5)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple supervised CBOW model in Keras\n",
    "\n",
    "The following computes a very simple model, as described in [fastText](https://github.com/facebookresearch/fastText):\n",
    "\n",
    "<img src=\"images/fasttext.svg\" style=\"width: 600px;\" />\n",
    "\n",
    "- Build an embedding layer mapping each word to a vector representation\n",
    "- Compute the vector representation of all words in each sequence and average them\n",
    "- Add a dense layer to output 20 classes (+ softmax)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:28:58.324450Z",
     "start_time": "2024-10-25T18:28:58.233955Z"
    }
   },
   "source": [
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "N_CLASSES = len(target_names)\n",
    "\n",
    "# input: a sequence of MAX_SEQUENCE_LENGTH integers\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "average = GlobalAveragePooling1D()(embedded_sequences)\n",
    "predictions = Dense(N_CLASSES, activation='softmax')(average)\n",
    "\n",
    "model = Model(sequence_input, predictions)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=0.01), metrics=['acc'])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:29:12.015432Z",
     "start_time": "2024-10-25T18:29:01.905857Z"
    }
   },
   "source": [
    "model.fit(x_train, y_train, validation_split=0.1,\n",
    "          epochs=10, batch_size=32)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 [==============================] - 2s 18ms/step - loss: 1.5973 - acc: 0.2640 - val_loss: 1.5833 - val_acc: 0.2759\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 1s 18ms/step - loss: 1.5614 - acc: 0.3227 - val_loss: 1.5488 - val_acc: 0.3695\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 1s 16ms/step - loss: 1.5148 - acc: 0.4682 - val_loss: 1.4991 - val_acc: 0.5172\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 1s 16ms/step - loss: 1.4515 - acc: 0.5554 - val_loss: 1.4265 - val_acc: 0.5468\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 1s 14ms/step - loss: 1.3701 - acc: 0.6158 - val_loss: 1.3403 - val_acc: 0.6158\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 1s 14ms/step - loss: 1.2768 - acc: 0.6894 - val_loss: 1.2468 - val_acc: 0.6897\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 1s 14ms/step - loss: 1.1765 - acc: 0.7184 - val_loss: 1.1481 - val_acc: 0.7340\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 1s 15ms/step - loss: 1.0733 - acc: 0.8063 - val_loss: 1.0524 - val_acc: 0.8177\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 1s 13ms/step - loss: 0.9719 - acc: 0.8436 - val_loss: 0.9578 - val_acc: 0.8424\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 1s 12ms/step - loss: 0.8764 - acc: 0.8814 - val_loss: 0.8692 - val_acc: 0.8818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ec2e207210>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercices**\n",
    "\n",
    "- Compute model accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:29:14.270163Z",
     "start_time": "2024-10-25T18:29:14.165324Z"
    }
   },
   "source": [
    "# Verify your solution by a tutor.\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 4ms/step - loss: 0.8963 - acc: 0.8400\n",
      "Test accuracy: 0.8399999737739563\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building more complex models\n",
    "\n",
    "**Exercise**\n",
    "- From the previous template, build more complex models using:\n",
    "  - **1d convolution and 1d maxpooling**. Note that you will still need a GloabalAveragePooling or Flatten after the convolutions as the final `Dense` layer expects a fixed size input;\n",
    "  - **Recurrent neural networks through LSTM** (you will need to **reduce sequence length before using the LSTM layer**).\n",
    "\n",
    "  \n",
    "<img src=\"images/unrolled_rnn_one_output_2.svg\" style=\"width: 600px;\" />\n",
    "\n",
    "**Bonus**\n",
    "- You may try different architectures with:\n",
    "  - more intermediate layers, combination of dense, conv, recurrent\n",
    "  - different recurrent (GRU, RNN)\n",
    "  - bidirectional LSTMs\n",
    "\n",
    "**Note**: The goal is to build working models rather than getting better test accuracy as this task is already very well solved by the simple model.  Build your model, and verify that they converge to OK results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:29:19.644788Z",
     "start_time": "2024-10-25T18:29:19.490440Z"
    }
   },
   "source": [
    "from keras.layers import Embedding, Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, LSTM, GRU\n",
    "from keras.layers import MaxPooling1D, GlobalAveragePooling1D \n",
    "from keras.models import Model\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "N_CLASSES = len(target_names)\n",
    "\n",
    "# input: a sequence of MAX_SEQUENCE_LENGTH integers\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "average = GlobalAveragePooling1D()(embedded_sequences)\n",
    "predictions = Dense(N_CLASSES, activation='softmax')(average)\n",
    "\n",
    "model = Model(sequence_input, predictions)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['acc'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:30:32.133141Z",
     "start_time": "2024-10-25T18:29:25.179624Z"
    }
   },
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "conv1d = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "maxpool = MaxPooling1D(5)(conv1d)\n",
    "average = GlobalAveragePooling1D()(maxpool)\n",
    "\n",
    "predictions = Dense(N_CLASSES, activation='softmax')(average)\n",
    "\n",
    "model = Model(sequence_input, predictions)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, validation_split=0.1,\n",
    "          epochs=15, batch_size=32)\n",
    "\n",
    "# Test accuracy\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy (Conv1D):\", test_accuracy)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "57/57 [==============================] - 6s 81ms/step - loss: 1.5641 - acc: 0.3282 - val_loss: 1.4812 - val_acc: 0.5320\n",
      "Epoch 2/15\n",
      "57/57 [==============================] - 4s 76ms/step - loss: 1.2166 - acc: 0.6608 - val_loss: 0.9058 - val_acc: 0.7340\n",
      "Epoch 3/15\n",
      "57/57 [==============================] - 4s 78ms/step - loss: 0.6263 - acc: 0.8699 - val_loss: 0.4238 - val_acc: 0.9557\n",
      "Epoch 4/15\n",
      "57/57 [==============================] - 4s 71ms/step - loss: 0.2811 - acc: 0.9654 - val_loss: 0.2197 - val_acc: 0.9803\n",
      "Epoch 5/15\n",
      "57/57 [==============================] - 4s 72ms/step - loss: 0.1374 - acc: 0.9874 - val_loss: 0.1385 - val_acc: 0.9704\n",
      "Epoch 6/15\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.0767 - acc: 0.9934 - val_loss: 0.1067 - val_acc: 0.9754\n",
      "Epoch 7/15\n",
      "57/57 [==============================] - 4s 76ms/step - loss: 0.0472 - acc: 0.9956 - val_loss: 0.0912 - val_acc: 0.9754\n",
      "Epoch 8/15\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.0304 - acc: 0.9995 - val_loss: 0.0771 - val_acc: 0.9803\n",
      "Epoch 9/15\n",
      "57/57 [==============================] - 4s 75ms/step - loss: 0.0208 - acc: 1.0000 - val_loss: 0.0724 - val_acc: 0.9754\n",
      "Epoch 10/15\n",
      "57/57 [==============================] - 4s 75ms/step - loss: 0.0152 - acc: 1.0000 - val_loss: 0.0675 - val_acc: 0.9754\n",
      "Epoch 11/15\n",
      "57/57 [==============================] - 4s 75ms/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.0663 - val_acc: 0.9754\n",
      "Epoch 12/15\n",
      "57/57 [==============================] - 4s 75ms/step - loss: 0.0089 - acc: 1.0000 - val_loss: 0.0672 - val_acc: 0.9803\n",
      "Epoch 13/15\n",
      "57/57 [==============================] - 4s 76ms/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0658 - val_acc: 0.9803\n",
      "Epoch 14/15\n",
      "57/57 [==============================] - 4s 77ms/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.0658 - val_acc: 0.9803\n",
      "Epoch 15/15\n",
      "57/57 [==============================] - 5s 82ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0638 - val_acc: 0.9803\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0873 - acc: 0.9700\n",
      "Test accuracy (Conv1D): 0.9700000286102295\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:35:40.181143Z",
     "start_time": "2024-10-25T18:35:39.628292Z"
    }
   },
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "# LSTM layer\n",
    "lstm = LSTM(64)(embedded_sequences)\n",
    "predictions = Dense(N_CLASSES, activation='softmax')(average)\n",
    "\n",
    "# Define and compile the model\n",
    "model = Model(sequence_input, predictions)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, validation_split=0.1,\n",
    "          epochs=15, batch_size=32)\n",
    "\n",
    "# Test accuracy\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy (LSTM):\", test_accuracy)\n",
    "\n"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 1000), dtype=tf.int32, name='input_6'), name='input_6', description=\"created by layer 'input_6'\") at layer \"embedding_5\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m predictions \u001B[38;5;241m=\u001B[39m Dense(N_CLASSES, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msoftmax\u001B[39m\u001B[38;5;124m'\u001B[39m)(average)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Define and compile the model\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m model \u001B[38;5;241m=\u001B[39m Model(sequence_input, predictions)\n\u001B[0;32m     13\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     14\u001B[0m               optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124macc\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\trackable\\base.py:204\u001B[0m, in \u001B[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 204\u001B[0m   result \u001B[38;5;241m=\u001B[39m method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    206\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m previous_value  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\functional.py:166\u001B[0m, in \u001B[0;36mFunctional.__init__\u001B[1;34m(self, inputs, outputs, name, trainable, **kwargs)\u001B[0m\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\n\u001B[0;32m    158\u001B[0m         [\n\u001B[0;32m    159\u001B[0m             functional_utils\u001B[38;5;241m.\u001B[39mis_input_keras_tensor(t)\n\u001B[0;32m    160\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mflatten(inputs)\n\u001B[0;32m    161\u001B[0m         ]\n\u001B[0;32m    162\u001B[0m     ):\n\u001B[0;32m    163\u001B[0m         inputs, outputs \u001B[38;5;241m=\u001B[39m functional_utils\u001B[38;5;241m.\u001B[39mclone_graph_nodes(\n\u001B[0;32m    164\u001B[0m             inputs, outputs\n\u001B[0;32m    165\u001B[0m         )\n\u001B[1;32m--> 166\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_graph_network(inputs, outputs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\trackable\\base.py:204\u001B[0m, in \u001B[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 204\u001B[0m   result \u001B[38;5;241m=\u001B[39m method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    206\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m previous_value  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\functional.py:265\u001B[0m, in \u001B[0;36mFunctional._init_graph_network\u001B[1;34m(self, inputs, outputs)\u001B[0m\n\u001B[0;32m    262\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_coordinates\u001B[38;5;241m.\u001B[39mappend((layer, node_index, tensor_index))\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# Keep track of the network's nodes and layers.\u001B[39;00m\n\u001B[1;32m--> 265\u001B[0m nodes, nodes_by_depth, layers, _ \u001B[38;5;241m=\u001B[39m _map_graph_network(\n\u001B[0;32m    266\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutputs\n\u001B[0;32m    267\u001B[0m )\n\u001B[0;32m    268\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_network_nodes \u001B[38;5;241m=\u001B[39m nodes\n\u001B[0;32m    269\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_nodes_by_depth \u001B[38;5;241m=\u001B[39m nodes_by_depth\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\functional.py:1145\u001B[0m, in \u001B[0;36m_map_graph_network\u001B[1;34m(inputs, outputs)\u001B[0m\n\u001B[0;32m   1143\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mflatten(node\u001B[38;5;241m.\u001B[39mkeras_inputs):\n\u001B[0;32m   1144\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mid\u001B[39m(x) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m computable_tensors:\n\u001B[1;32m-> 1145\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1146\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGraph disconnected: cannot obtain value for \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1147\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtensor \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m at layer \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   1148\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe following previous layers were accessed \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1149\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwithout issue: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayers_with_complete_input\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1150\u001B[0m         )\n\u001B[0;32m   1151\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mflatten(node\u001B[38;5;241m.\u001B[39moutputs):\n\u001B[0;32m   1152\u001B[0m     computable_tensors\u001B[38;5;241m.\u001B[39madd(\u001B[38;5;28mid\u001B[39m(x))\n",
      "\u001B[1;31mValueError\u001B[0m: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 1000), dtype=tf.int32, name='input_6'), name='input_6', description=\"created by layer 'input_6'\") at layer \"embedding_5\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:36:56.093020Z",
     "start_time": "2024-10-25T18:35:51.491162Z"
    }
   },
   "source": [
    "model.fit(x_train, y_train, validation_split=0.1,\n",
    "          epochs=15, batch_size=32)\n",
    "\n",
    "output_test = model(x_test)\n",
    "test_casses = np.argmax(output_test, axis=-1)\n",
    "print(\"Test accuracy:\", np.mean(test_casses == target_test))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "57/57 [==============================] - 4s 75ms/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0630 - val_acc: 0.9803\n",
      "Epoch 2/15\n",
      "57/57 [==============================] - 4s 73ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0642 - val_acc: 0.9803\n",
      "Epoch 3/15\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0641 - val_acc: 0.9803\n",
      "Epoch 4/15\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0641 - val_acc: 0.9803\n",
      "Epoch 5/15\n",
      "57/57 [==============================] - 4s 78ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0636 - val_acc: 0.9803\n",
      "Epoch 6/15\n",
      "57/57 [==============================] - 4s 76ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0636 - val_acc: 0.9803\n",
      "Epoch 7/15\n",
      "57/57 [==============================] - 4s 77ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0642 - val_acc: 0.9803\n",
      "Epoch 8/15\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0648 - val_acc: 0.9803\n",
      "Epoch 9/15\n",
      "57/57 [==============================] - 4s 70ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0645 - val_acc: 0.9803\n",
      "Epoch 10/15\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0657 - val_acc: 0.9803\n",
      "Epoch 11/15\n",
      "57/57 [==============================] - 4s 73ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0644 - val_acc: 0.9803\n",
      "Epoch 12/15\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0653 - val_acc: 0.9803\n",
      "Epoch 13/15\n",
      "57/57 [==============================] - 5s 79ms/step - loss: 9.8639e-04 - acc: 1.0000 - val_loss: 0.0651 - val_acc: 0.9803\n",
      "Epoch 14/15\n",
      "57/57 [==============================] - 4s 77ms/step - loss: 9.0562e-04 - acc: 1.0000 - val_loss: 0.0652 - val_acc: 0.9803\n",
      "Epoch 15/15\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 8.2967e-04 - acc: 1.0000 - val_loss: 0.0660 - val_acc: 0.9803\n",
      "Test accuracy: 0.97\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pre-trained embeddings\n",
    "\n",
    "The file `glove100K.100d.txt` is an extract of [Glove](http://nlp.stanford.edu/projects/glove/) Vectors, that were trained on english Wikipedia 2014 + Gigaword 5 (6B tokens).\n",
    "\n",
    "We extracted the `100 000` most frequent words. They have a dimension of `100`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T20:18:24.777380Z",
     "start_time": "2024-10-25T20:18:24.068950Z"
    }
   },
   "source": [
    "embeddings_index = {}\n",
    "embeddings_vectors = []\n",
    "with open('glove100K.100d.txt', 'rb') as f:\n",
    "    word_idx = 0\n",
    "    for line in f:\n",
    "        values = line.decode('utf-8').split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = word_idx\n",
    "        embeddings_vectors.append(vector)\n",
    "        word_idx = word_idx + 1\n",
    "\n",
    "inv_index = {v: k for k, v in embeddings_index.items()}\n",
    "print(\"found %d different words in the file\" % word_idx)"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove100K.100d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m embeddings_index \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m      2\u001B[0m embeddings_vectors \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mglove100K.100d.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m      4\u001B[0m     word_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m f:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    319\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    322\u001B[0m     )\n\u001B[1;32m--> 324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'glove100K.100d.txt'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all embeddings in a large numpy array\n",
    "glove_embeddings = np.vstack(embeddings_vectors)\n",
    "glove_norms = np.linalg.norm(glove_embeddings, axis=-1, keepdims=True)\n",
    "glove_embeddings_normed = glove_embeddings / glove_norms\n",
    "print(glove_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(word):\n",
    "    idx = embeddings_index.get(word)\n",
    "    if idx is None:\n",
    "        return None\n",
    "    else:\n",
    "        return glove_embeddings[idx]\n",
    "\n",
    "    \n",
    "def get_normed_emb(word):\n",
    "    idx = embeddings_index.get(word)\n",
    "    if idx is None:\n",
    "        return None\n",
    "    else:\n",
    "        return glove_embeddings_normed[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_emb(\"computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding most similar words\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "Build a function to find most similar words, given a word as query:\n",
    "- lookup the vector for the query word in the Glove index;\n",
    "- compute the cosine similarity between a word embedding and all other words;\n",
    "- display the top 10 most similar words.\n",
    "\n",
    "**Bonus**\n",
    "\n",
    "Change your function so that it takes multiple words as input (by averaging them)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:38:05.082438Z",
     "start_time": "2024-10-25T18:38:05.061373Z"
    }
   },
   "source": [
    "# Verify your solution by a tutor.\n",
    "def most_similar(word, top_n=10):\n",
    "    word_emb = get_normed_emb(word)\n",
    "    if word_emb is None:\n",
    "        return []\n",
    "    similarity = np.dot(glove_embeddings_normed, word_emb)\n",
    "    similar_idxs = np.argsort(similarity)[::-1][:top_n]\n",
    "    return [(inv_index[i], similarity[i]) for i in similar_idxs]\n"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:38:13.409892Z",
     "start_time": "2024-10-25T18:38:13.363956Z"
    }
   },
   "source": [
    "most_similar(\"cpu\")"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_normed_emb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[41], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m most_similar(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[40], line 3\u001B[0m, in \u001B[0;36mmost_similar\u001B[1;34m(word, top_n)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmost_similar\u001B[39m(word, top_n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m):\n\u001B[1;32m----> 3\u001B[0m     word_emb \u001B[38;5;241m=\u001B[39m get_normed_emb(word)\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m word_emb \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m      5\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m []\n",
      "\u001B[1;31mNameError\u001B[0m: name 'get_normed_emb' is not defined"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(\"pitt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(\"jolie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the future better than tarot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(get_normed_emb('aniston'), get_normed_emb('pitt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(get_normed_emb('jolie'), get_normed_emb('pitt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bonus: sum of two word embeddings\n",
    "most_similar([\"river\", \"chinese\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying vectors with  t-SNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "word_emb_tsne = TSNE(perplexity=30).fit_transform(glove_embeddings_normed[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(40, 40))\n",
    "axis = plt.gca()\n",
    "np.set_printoptions(suppress=True)\n",
    "plt.scatter(word_emb_tsne[:, 0], word_emb_tsne[:, 1], marker=\".\", s=1)\n",
    "\n",
    "for idx in range(1000):\n",
    "    plt.annotate(inv_index[idx],\n",
    "                 xy=(word_emb_tsne[idx, 0], word_emb_tsne[idx, 1]),\n",
    "                 xytext=(0, 0), textcoords='offset points')\n",
    "plt.savefig(\"tsne.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre-trained embeddings in our model\n",
    "\n",
    "We want to use these pre-trained embeddings for transfer learning. This process is rather similar than transfer learning in image recognition: the features learnt on words might help us bootstrap the learning process, and increase performance if we don't have enough training data.\n",
    "- We initialize embedding matrix from the model with Glove embeddings:\n",
    " - take all unique words from our BBC news dataset to build a vocabulary (`MAX_NB_WORDS = 20000`), and look up their Glove embedding \n",
    " - place the Glove embedding at the corresponding index in the matrix\n",
    " - if the word is not in the Glove vocabulary, we only place zeros in the matrix\n",
    "- We may fix these embeddings or fine-tune them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# prepare embedding matrix\n",
    "nb_words_in_matrix = 0\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = get_emb(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        nb_words_in_matrix = nb_words_in_matrix + 1\n",
    "        \n",
    "print(\"added %d words in the embedding matrix\" % nb_words_in_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a layer with pre-trained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding_layer = Embedding(\n",
    "    MAX_NB_WORDS, EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A model with pre-trained Embeddings\n",
    "\n",
    "Average word embeddings pre-trained with Glove / Word2Vec usually works surprisingly well. However, when averaging more than `10-15` words, the resulting vector becomes too noisy and classification performance is degraded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = pretrained_embedding_layer(sequence_input)\n",
    "average = GlobalAveragePooling1D()(embedded_sequences)\n",
    "predictions = Dense(N_CLASSES, activation='softmax')(average)\n",
    "\n",
    "model = Model(sequence_input, predictions)\n",
    "\n",
    "# We don't want to fine-tune embeddings\n",
    "model.layers[1].trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=0.01), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_split=0.1,\n",
    "          epochs=15, batch_size=32)\n",
    "loss, accuracy = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks:**\n",
    "\n",
    "- On this type of task, using pre-trained embeddings can degrade results as we train much less parameters and we average a large number pre-trained embeddings.\n",
    "\n",
    "- Pre-trained embeddings followed by global averaging prevents overfitting but can also cause some underfitting.\n",
    "\n",
    "- Using convolutions / LSTM should help counter the underfitting effect.\n",
    "\n",
    "- It is also advisable to treat separately pre-trained embeddings and words out of vocabulary.\n",
    "\n",
    "Pre-trained embeddings can be very useful when the training set is small and the individual text documents to classify are short: in this case there might be a single very important word in a test document that drives the label. If that word has never been seen in the training set but some synonyms were seen, the semantic similarity captured by the embedding will allow the model to generalized out of the restricted training set vocabulary.\n",
    "\n",
    "We did not observe this effect here because the document are long enough so that guessing the topic can be done redundantly. Shortening the documents to make the task more difficult could possibly highlight this benefit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reality check\n",
    "\n",
    "On small/medium datasets (few 10,000s) of reasonably large documents (e.g. more than a few paragraphs), simpler classification methods usually perform better, and are much more efficient to train and use. Here are two resources to go further:\n",
    "- Naive Bayes approach, using scikit-learn http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n",
    "- Alec Radford (OpenAI) gave a very interesting presentation, showing that you need a VERY large dataset to have real gains from GRU/LSTM in text classification https://www.slideshare.net/odsc/alec-radfordodsc-presentation\n",
    "\n",
    "Training deep architectures from random init on text classification is usually a waste of time.\n",
    "\n",
    "However, when looking at features, one can see that classification using simple methods isn't very robust, and won't generalize well to slightly different domains (e.g. forum posts => emails)\n",
    "\n",
    "Nowadays, the strategy would be to use pre-trained deep network (BERT) to extract features and fit a linear classifer on top of this. This is especially useful when classifying short texts (e.g. one or a few sentences) as this kind of tasks can be very sensitive to understanding the meaning resulting from intra-sentence interactions between words. The next session on attentional mechanisms and pre-trained transformer-based word models will explain this in more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "- Compare pre-trained embeddings vs specifically trained embeddings\n",
    "- Train your own wordvectors in any language using [gensim's word2vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "- Check [Keras Examples](https://github.com/fchollet/keras/tree/master/examples) on `imdb` sentiment analysis\n",
    "- Install fastText (Linux or macOS only, use the Linux VM if under Windows) and give it a try on the classification example in its repository.\n",
    "- Today, the **state-of-the-art text classification** can be achieved by **transfer learning from a language model** instead of using traditional word embeddings. See for instance: [ULMFit, Fine-tuned Language Models for Text Classification](https://arxiv.org/abs/1801.06146), [ELMO](https://allennlp.org/elmo), [GPT](https://blog.openai.com/language-unsupervised/), [BERT](https://arxiv.org/abs/1810.04805), [GPT-2](https://github.com/openai/gpt-2). The second notebook introduces how to train such a language model from unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
